{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "uwskyl2ickjotzteiko6",
   "authorId": "154296475017",
   "authorName": "DSHAW_SFC",
   "authorEmail": "diana.shaw@snowflake.com",
   "sessionId": "5acc5de6-8e3b-4693-ba67-0103c9ec8cba",
   "lastEditTime": 1747153634566
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "Imports"
   },
   "source": "import warnings\nwarnings.filterwarnings(\"ignore\")\nimport streamlit as st\nimport pandas as pd\nimport json\nimport re\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import col\nfrom snowflake.snowpark import types as T\nfrom snowflake.snowpark import Row\nfrom snowflake.core import Root\nfrom snowflake.cortex import Complete\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "NotebookOverview",
    "collapsed": false,
    "codeCollapsed": true
   },
   "source": "# Define image in a stage and read the file\nimage=session.file.get_stream('@aicollege.public.transcripts/Phase2.jpg' , decompress=False).read() \n\n# Display the image\nst.image(image, width=400)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7dacb431-c209-4838-8354-1028ca344ffc",
   "metadata": {
    "language": "sql",
    "name": "ViewStage"
   },
   "outputs": [],
   "source": "-- View the list of files in the stage to ensure 6 are present\nLIST @AICOLLEGE.PUBLIC.TRANSCRIPTS PATTERN='.*\\.pdf$';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34d0755a-992b-4319-99d4-568bbc4b67f7",
   "metadata": {
    "language": "sql",
    "name": "ViewTranscript"
   },
   "outputs": [],
   "source": "-- View a transcript\nSELECT\n  SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n    '@AICOLLEGE.PUBLIC.TRANSCRIPTS',\n    'Acme Corp Services Discovery Meeting Transcript.pdf',\n    OBJECT_CONSTRUCT('mode','layout')\n  ) AS layout;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "61a854a7-cd27-4efa-a687-5b6a525898ad",
   "metadata": {
    "name": "PARSE_DOCUMENT",
    "collapsed": false
   },
   "source": "### üßæ Extract PDF Transcript Content with `PARSE_DOCUMENT`\n\nWe‚Äôll use the [`PARSE_DOCUMENT`](https://docs.snowflake.com/en/sql-reference/functions/parse_document-snowflake-cortex) function to extract the full contents of each transcript PDF.\n\nSnowflake has simplified working with unstructured documents by providing a Cortex AI SQL function that returns the extracted content as a structured JSON object ‚Äî no external parsing tools required.\n\nThis is the **first step** in working with unstructured data, enabling:\n- RAG pipelines powered by Cortex Search\n- LLM workflows like summarization, translation, or classification\n- Structured output extraction from forms and contracts\n\nüí° `PARSE_DOCUMENT` supports two modes:\n- **OCR mode** ‚Üí Best for clean text extraction from scanned documents  \n- **LAYOUT mode** ‚Üí Best for preserving structure like tables, headers, and sections (used here)\n\nWe‚Äôll store the extracted content along with customer name and file path in a `PARSED_TRANSCRIPTS` table for future use."
  },
  {
   "cell_type": "code",
   "id": "d1860fdc-701c-44b3-83c8-4b0bfc62ee56",
   "metadata": {
    "language": "sql",
    "name": "ParseTranscripts"
   },
   "outputs": [],
   "source": "-- Create the parsed_transcripts table\nCREATE OR REPLACE TABLE PARSED_TRANSCRIPTS (\n    CUSTOMER_NAME  STRING,                      -- e.g. ‚ÄúAcme Corp Services‚Äù\n    RELATIVE_PATH  STRING PRIMARY KEY,          -- full path in the stage\n    RAW_TEXT       VARIANT,                     -- PDF text\n    LOADED_AT      TIMESTAMP_TZ DEFAULT CURRENT_TIMESTAMP()\n);\n\n-- Process files one at a time to avoid stage scanning issues\nINSERT INTO PARSED_TRANSCRIPTS (CUSTOMER_NAME, RELATIVE_PATH, RAW_TEXT)\nSELECT \n    REGEXP_REPLACE(RELATIVE_PATH,' Discovery Meeting Transcript\\\\.pdf$','') AS CUSTOMER_NAME,\n    RELATIVE_PATH,\n    SNOWFLAKE.XXX(      --> Use Snowflake's Parse Document function\n        '@AICOLLEGE.PUBLIC.TRANSCRIPTS',\n        RELATIVE_PATH,\n        OBJECT_CONSTRUCT('mode','XXX') --> Use layout mode\n    ) AS RAW_TEXT\nFROM DIRECTORY('@AICOLLEGE.PUBLIC.TRANSCRIPTS') f\nWHERE\n  RELATIVE_PATH LIKE '%Discovery Meeting Transcript.pdf'\n  AND RELATIVE_PATH NOT IN (SELECT RELATIVE_PATH FROM PARSED_TRANSCRIPTS);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "75324729-c49d-47dd-af2e-965c81a04420",
   "metadata": {
    "language": "sql",
    "name": "SeeParsedText"
   },
   "outputs": [],
   "source": "-- Validate PARSED_TRANSCRIPTS was created correctly\nSELECT * FROM PARSED_TRANSCRIPTS LIMIT 5;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6b8251a8-8380-4a86-a8b2-d13ea6cef003",
   "metadata": {
    "name": "SPLIT_TEXT_RECURSIVE_CHARACTER",
    "collapsed": false
   },
   "source": "### ‚úÇÔ∏è Chunk Transcript Text with SPLIT_TEXT_RECURSIVE_CHARACTER\nOnce we‚Äôve extracted the full layout of each transcript, the next step is to split the long text into smaller overlapping chunks. This is a critical step before embedding or indexing for semantic search.\n\nWe‚Äôll use the [`SPLIT_TEXT_RECURSIVE_CHARACTER`](https://docs.snowflake.com/en/sql-reference/functions/split_text_recursive_character-snowflake-cortex) function, which is designed to split long documents intelligently based on a preferred delimiter (e.g., paragraph breaks).\n\nThis prepares the data for downstream use with:\n- Cortex Search indexing\n- Embedding generation\n- Summarization or classification with LLMs\n\nüí° We‚Äôre using:\n- A chunk size of **800 characters**\n- An overlap of **150 characters**\n- A preferred break on \"\\n\\n\" to preserve paragraph structure"
  },
  {
   "cell_type": "code",
   "id": "be605bb5-63d0-43b1-88ac-6b832aae107f",
   "metadata": {
    "language": "sql",
    "name": "ChunkTranscripts"
   },
   "outputs": [],
   "source": "-- Create chunks of the parsed transcripts\nCREATE OR REPLACE TABLE TRANSCRIPT_CHUNKS (\n    CUSTOMER_NAME  STRING,\n    RELATIVE_PATH  STRING,\n    CHUNK          STRING\n);\n\n-- Insert chunked content using SPLIT_TEXT_RECURSIVE_CHARACTER\nINSERT INTO TRANSCRIPT_CHUNKS (CUSTOMER_NAME, RELATIVE_PATH, CHUNK)\nWITH text_chunks AS (\n    SELECT\n        CUSTOMER_NAME,\n        RELATIVE_PATH,\n        SNOWFLAKE.XXX(  --> Use Snowflake's Split Text Recursive function\n            RAW_TEXT:content::STRING,  -- extract string content\n            'markdown',                -- tokenizer\n            800,                       -- chunk size\n            100,                       -- overlap\n            ARRAY_CONSTRUCT('\\n\\n')    -- preferred break\n        ) AS CHUNKS\n    FROM PARSED_TRANSCRIPTS\n    WHERE RAW_TEXT:content IS NOT NULL -- optional safety check\n)\nSELECT\n    CUSTOMER_NAME,\n    RELATIVE_PATH,\n    chunk.value::STRING AS XXX  --> Save chunks of parsed transcripts as \"CHUNK\"\nFROM text_chunks,\nLATERAL FLATTEN(input => CHUNKS) AS chunk;  --> Flatten the chuncked output",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e19e0835-5c5d-4579-856a-858fdae729a5",
   "metadata": {
    "language": "sql",
    "name": "SeeChunkedText"
   },
   "outputs": [],
   "source": "-- Validate TRANSCRIPT_CHUNKS was created correctly\nSELECT * FROM TRANSCRIPT_CHUNKS LIMIT 5;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "249d037c-8049-4b3e-9a25-cd8b3c92077d",
   "metadata": {
    "name": "CORTEX_SEARCH_SERVICES",
    "collapsed": false
   },
   "source": "### üîç Make Your Transcripts Searchable with Cortex Search\n\nNow that we've extracted and chunked the content of each transcript, the next step is to make it **searchable** using Snowflake's fully managed retrieval system: **Cortex Search**.\n\n[`CORTEX SEARCH SERVICE`](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-search/cortex-search-overview) enables **hybrid retrieval** ‚Äî combining vector search (semantic meaning) with keyword search (lexical matching) ‚Äî to deliver **highly accurate results** out of the box.\n\nThis capability is a key enabler for Retrieval-Augmented Generation (RAG), allowing you to:\n- Ask contextual questions over your documents using SQL or LLM interfaces\n- Power chatbots and copilots with document-grounded responses\n- Serve precise and explainable enterprise search across large volumes of unstructured content\n\nüí° Powered by `snowflake-arctic-embed-l-v2.0`, a state-of-the-art embedding model optimized for enterprise search.\n\nOnce the search service is defined, you can start querying it with natural language ‚Äî directly inside Snowflake ‚Äî without needing external pipelines, vector DBs, or hosting infrastructure."
  },
  {
   "cell_type": "code",
   "id": "a8e9af90-1d61-4c26-bd40-d24e2abdb14b",
   "metadata": {
    "language": "sql",
    "name": "CreateCortexSearchService"
   },
   "outputs": [],
   "source": "-- Create a Cortex Search Service over your chunked transcripts table\nCREATE OR REPLACE XXX AICOLLEGE.PUBLIC.TRANSCRIPTS_SEARCH_SERVICE  --> Create \"TRANSCRIPTS_SEARCH_SERVICE\" via Snowflake's Cortex Search Service hybrid retrieval function\n    ON CHUNK\n    ATTRIBUTES CUSTOMER_NAME, RELATIVE_PATH\n    WAREHOUSE = AICOLLEGE\n    TARGET_LAG = '365 days'\n    EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'  -- Optional: Select your desired embedding model\n    AS (\n    SELECT\n        CUSTOMER_NAME,\n        RELATIVE_PATH,\n        CHUNK::VARCHAR AS CHUNK,\n    FROM TRANSCRIPT_CHUNKS);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6eb4d5ff-3d60-4d65-bd50-b76a47db2df0",
   "metadata": {
    "name": "SimpleRAGPipeline",
    "collapsed": false
   },
   "source": "### Test your service ‚Äî Baseline **‚Äúone-shot‚Äù RAG** (even with a top-tier model)\n\nSelect a customer and then run the *BaselineRAGPipeline* cell to perform the **simplest possible Retrieval-Augmented Generation flow**:\n\n1. **Retrieve** the **first** transcript chunk that semantically matches the user‚Äôs question.  \n2. **Generate** an answer by stuffing that single chunk into the prompt of a **state-of-the-art model** (*`claude-3-5-sonnet`* or *`mistral-large2`*).\n\n---\n\n> Even with a premium LLM you‚Äôll notice it can only repeat whatever facts happen to live in that lone chunk.  \n> It frequently replies with **‚ÄúBased on the limited context provided ‚Ä¶‚Äù** because:\n>\n> * The relevant details might sit in a **different** chunk.  \n> * We pass **no metadata** (e.g., speaker tags, meeting header) to help the model reason.  \n> * We don‚Äôt ask it to **extract** or **structure** anything.\n>\n> You‚Äôll address all of these gaps in the next steps."
  },
  {
   "cell_type": "code",
   "id": "def68695-2e65-405e-9a89-8bea54f3193d",
   "metadata": {
    "language": "python",
    "name": "SelectCustomer",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Run this and then select a distinct customer from the dropdown list\ncustomers = [r[\"CUSTOMER_NAME\"] for r in session.table(\"CUSTOMER_INSIGHTS\").select(\"CUSTOMER_NAME\").distinct().collect()]\n\ncustomers = sorted(customers)\n\nselected = st.selectbox(\n    \"Run this cell, then select a customer to analyze\",\n    options=customers,\n    key=\"selected_customer\"\n)\n\nst.success(f\"Chosen customer: {st.session_state.selected_customer}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "056d319b-65aa-4503-92cb-50efd579adcc",
   "metadata": {
    "language": "python",
    "name": "BaselineRAGPipeline"
   },
   "outputs": [],
   "source": "# Create service handle\nroot = Root(get_active_session())\ncustomer = st.session_state.selected_customer\nsvc  = root.databases[\"AICOLLEGE\"]\\\n           .schemas[\"PUBLIC\"]\\\n           .cortex_search_services[\"XXX\"]  # --> Use your Snowflake Cortex Search Service\n\n# Specify user question\nquestion = f\"\"\"\n            What was the meeting with {customer} about? \n            Who attended? When was the meeting? What is the next step?\n            \"\"\".strip()\n\n# Get naive retrieval ‚Äì grab the very first hit (could be just the title page)\nhit = svc.search(\n        query            = question,\n        columns          = [\"CUSTOMER_NAME\", \"CHUNK\", \"RELATIVE_PATH\"],\n        limit            = 5,                # look at a few chunks\n        search_type      = \"embed\"           # basic semantic search\n     ).results[0]\n\nst.info(f\"**File:** {hit['RELATIVE_PATH']}\\n\\n{hit['CHUNK']}\")\n\n# Single-pass generation\nresponse = Complete(\n             model  = \"XXX\",  # --> use Anthropic model or Mistral Large model\n             prompt = (\n                 f\"{question}\\n\\n\"\n                 \"Answer **only** using the context below:\\n\"\n                 \"-----\\n\"\n                 f\"{hit['CHUNK']}\\n\"\n                 \"-----\"\n             )\n           ).strip()\n\nst.info(f\"**LLM Response:**\\n\\n{response}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "478861a6-d49e-4666-9c78-4b859d76dea7",
   "metadata": {
    "name": "EnhancedRAGPipeline",
    "collapsed": false
   },
   "source": "### üß® From ‚Äúone-shot‚Äù to üéØ **Production-ready**: the 3-Step Enhanced RAG Pipeline  \n\n| Step | What happens | Why it matters |\n|------|--------------|----------------|\n| **1&nbsp;¬∑&nbsp;Smart Retrieval** *(hybrid search + customer filter + top-k)* | We ask `TRANSCRIPTS_SEARCH_SERVICE` for the **five** best chunks that match the user‚Äôs question **and** the selected customer.  Hybrid mode blends dense-vector recall *and* keyword precision; score / size filters keep the text relevant and compact. | Pulls **all** salient passages (not just the first hit) while avoiding noise. Higher recall ‚áí fewer ‚Äúmissing-context‚Äù answers and less hallucination. |\n| **2&nbsp;¬∑&nbsp;Schema-Driven Extraction** *(LLM returns strict JSON)* | A purpose-built prompt instructs the LLM to emit **only** `meeting_date, attendees, agenda, next_step` as JSON.  The prompt includes the concatenated chunks with clear separators. | Guarantees a predictable shape you can trust in code.  Eliminates manual parsing.  Minimises off-topic text & hallucinations. |\n| **3&nbsp;¬∑&nbsp;Persist Results** *(Snowpark DataFrame ‚Üí `TRANSCRIPT_FACTS`)* | The JSON is converted to a Snowpark DF and saved as a table.  Each row is enriched with `customer_name` & `relative_path` so you know exactly where it came from. | Makes the extracted facts **queryable in SQL** immediately. We will JOIN it with your Phase 1 tables or add it to your semantic model.|\n\n---\n\n### Why this 3-Step Enhanced RAG Pipeline beats the ‚Äúone-shot‚Äù Baseline\n\n* **More context ‚â† more noise** ‚Äì Hybrid retrieval surfaces the right chunks, not just the first one.  \n* **LLM as parser**, not essayist ‚Äì A tight schema prompt forces the model to structure data instead of rambling.  \n* **SQL-ready output** ‚Äì Persisting to a temp table lets analysts (or Cortex Analyst) treat unstructured insights just like any other table‚Äîno copy-paste required.\n\n> **Bottom line:** this 3-step pipeline turns messy PDF transcripts into reliable, query-ready facts for efficient and accurate results."
  },
  {
   "cell_type": "code",
   "id": "dc238a9e-4ce1-4f64-9408-15fa35841005",
   "metadata": {
    "language": "python",
    "name": "HybridSearch"
   },
   "outputs": [],
   "source": "# Build better retrieval (chunk filter + hybrid search) to retrieve top-k chunks for Acme Corp\nhits = svc.search(\n    query           = question,\n    columns         = [\"CUSTOMER_NAME\", \"CHUNK\", \"RELATIVE_PATH\"],\n    limit           = 4,                 # grab more context\n    search_type     = \"hybrid\",           # embedding + keyword\n    filters         = {\"CUSTOMER_NAME\": [customer]},  # force-match\n    min_score       = 0.15,              # drop low-relevance chunks\n    max_chunk_chars = 400                # trim huge blobs\n).results\n\n# join them into one context string\ncontext = \"\\n\\n---\\n\\n\".join(f\"[{i+1}] {h['CHUNK']}\" for i,h in enumerate(hits))\nst.info(context)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d88ac95-2e8d-45ca-b475-c270b716cc27",
   "metadata": {
    "language": "python",
    "name": "StructuredJSONExtraction"
   },
   "outputs": [],
   "source": "# Build structured extraction prompt to return strict JSON so it's easier to load into a table\nschema_prompt = f\"\"\"\nYou are a Snowflake analyst. From the context below extract ONLY these fields:\nmeeting_date, attendees, agenda, next_step.\nReturn a JSON object with exactly those keys. If a field is missing, use null.\n\nContext:\n<<<\n{context}\n>>>\n\"\"\".strip()\n\n# call the LLM (no response_format kwarg on this API)\nreply_text = XXX(              # --> Use the Snowflake Complete function\n    model  = \"claude-3-5-sonnet\",   # use Anthropic model or Mistral Large model\n    prompt = schema_prompt\n).strip()\n\n# parse the JSON\nfacts = json.loads(reply_text)\n\n# enrich\nfacts[\"customer_name\"]  = customer\nfacts[\"relative_path\"]  = hits[0][\"RELATIVE_PATH\"]\n\n# show it\nst.json(facts)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab8fed0e-593a-432b-914b-103c50cc1d23",
   "metadata": {
    "language": "python",
    "name": "PersistExtractText"
   },
   "outputs": [],
   "source": "# Persist extracted facts into a table with explicit schema\n# Normalise every field that must be STRING\ndef to_text(v):\n    \"\"\"Convert list/tuple/dict/None to a readable string for Snowflake.\"\"\"\n    if v is None:\n        return \"\"\n    if isinstance(v, (list, tuple)):\n        # Flatten nested lists first\n        flat = []\n        for x in v:\n            if isinstance(x, (list, tuple)):\n                flat.extend(x)\n            else:\n                flat.append(x)\n        return \"; \".join(str(x) for x in flat)\n    if isinstance(v, dict):\n        return json.dumps(v, ensure_ascii=False)\n    # already scalar (str, int, float, etc.)\n    return str(v)\n\nfacts[\"agenda\"]     = to_text(facts.get(\"agenda\"))\nfacts[\"next_step\"]  = to_text(facts.get(\"next_step\"))\n\n# Ensure attendees is a clean array of strings\natt = facts.get(\"attendees\")\nif isinstance(att, dict):\n    facts[\"attendees\"] = [v for _, v in sorted(att.items(), key=lambda kv: int(kv[0]))]\nelif isinstance(att, (list, tuple)):\n    facts[\"attendees\"] = [str(a) for a in att]          # coerce to str\nelse:\n    facts[\"attendees\"] = [str(att)] if att is not None else []\n\n# Define the Snowpark schema\nfact_schema = T.StructType([\n    T.StructField(\"meeting_date\",    T.StringType()),\n    T.StructField(\"attendees\",       T.ArrayType(T.StringType())),\n    T.StructField(\"agenda\",          T.StringType()),\n    T.StructField(\"next_step\",       T.StringType()),\n    T.StructField(\"customer_name\",   T.StringType()),\n    T.StructField(\"relative_path\",   T.StringType())\n])\n\n# Create the DataFrame\nsnow_df = session.create_dataframe([facts], schema=fact_schema)\n\n# Display in Streamlit\nst.markdown(\"**Structured meeting facts:**\")\nst.dataframe(snow_df.to_pandas())\n\n# Save as table\nsnow_df.write.save_as_table(\"XXX\", mode=\"overwrite\", table_type=\"XXX\")  # --> Create a TEMPORARY table called \"TRANSCRIPT_FACTS\"\n\nst.success(\"‚úÖ Facts saved to table `TRANSCRIPT_FACTS`\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1a36b9e5-c37c-4edf-b684-14f23870cb03",
   "metadata": {
    "name": "MoreUnstructuredExtractEnahancements",
    "collapsed": false
   },
   "source": "### Enrich Your Transcript Facts with Sentiment, Key Phrases & Risk Scoring\n\nBefore you run the batch pipeline, let‚Äôs add three powerful new columns to your `TRANSCRIPT_FACTS` table:\n\n1. **`TRANSCRIPT_SENTIMENT`** ‚Äì A 0.0‚Äì1.0 score capturing the overall tone of the **customer‚Äôs dialogue** (timestamps only).  \n2. **`KEY_PHRASES`** ‚Äì An array of distilled phrases representing the customer‚Äôs top **use cases**, **pain points**, or **requirements**.  \n3. **`RISK_SCORING`** ‚Äì A 0.0‚Äì1.0 numeric indicator of **deal risk**, as assessed by the LLM based on customer language.\n\nThese enrichments let you:\n\n- **Track sentiment** by customer or region to spot relationship health.  \n- **Filter or group** by specific key phrases (e.g. ‚ÄúSnowpark POC‚Äù, ‚Äúdata governance‚Äù).  \n- **Prioritize follow-ups** by sorting deals by risk score. "
  },
  {
   "cell_type": "code",
   "id": "d4c2ef7d-c6e8-41a8-8843-7246735eaa06",
   "metadata": {
    "language": "sql",
    "name": "MoreEnhancements"
   },
   "outputs": [],
   "source": "ALTER TABLE TRANSCRIPT_FACTS \nADD (\n    TRANSCRIPT_SENTIMENT DOUBLE,\n    KEY_PHRASES ARRAY,\n    RISK_SCORING FLOAT\n);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6924f063-a309-43f1-9fdb-3dfc2ab11a28",
   "metadata": {
    "language": "python",
    "name": "TranscriptSentiment"
   },
   "outputs": [],
   "source": "# Unpack identifiers (from your JSON‚Äêextraction cell) \ncustomer      = facts[\"customer_name\"]\nrelative_path = facts[\"relative_path\"]\nmeeting_date  = facts.get(\"meeting_date\")   # might be None\n\n# Regex to find a ‚Äúhh:mm‚Äù timestamp in the chunk\nts_re = re.compile(r\"\\b\\d{1,2}:\\d{2}\\b\")\n\ndialogue_chunks = []\nfor h in hits:  \n    text = h[\"CHUNK\"]\n    m = ts_re.search(text)\n    if m:\n        # grab from the first timestamp through the end\n        dialogue_chunks.append(text[m.start():])\n    # else: skip this chunk entirely (it was probably header/overview)\n\n# Fallback if *none* of your hits had timestamps\nif dialogue_chunks:\n    transcript_text = \"\\n\\n\".join(dialogue_chunks)\nelse:\n    # last resort: join all hits (you will see header, but at least you get something)\n    transcript_text = \"\\n\\n\".join(h[\"CHUNK\"] for h in hits)\n\nst.info(f\"‚úÇÔ∏è Snippet of transcript used for sentiment (only first 500 chars shown):\\n\\n{transcript_text[:500]}‚Ä¶\")\n\n# Score via a single CORTEX.SENTIMENT() call\nsent_score = session.sql(\n    \"SELECT SNOWFLAKE.XXX(?) AS S\",   # --> Use the Snowflake Cortex Sentiment function\n    [transcript_text]\n).collect()[0][\"S\"]\n\n# Write back into TRANSCRIPT_FACTS.transcript_sentiment\nif meeting_date is not None:\n    upd = \"\"\"\n      UPDATE AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS\n         SET transcript_sentiment = ?\n       WHERE relative_path = ?\n         AND customer_name = ?\n         AND meeting_date  = ?\n    \"\"\"\n    params = [float(sent_score), relative_path, customer, meeting_date]\nelse:\n    upd = \"\"\"\n      UPDATE AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS\n         SET transcript_sentiment = ?\n       WHERE relative_path = ?\n         AND customer_name = ?\n    \"\"\"\n    params = [float(sent_score), relative_path, customer]\n\nsession.sql(upd, params).collect()\nst.success(f\"‚úÖ transcript_sentiment = {sent_score:.3f} saved to TRANSCRIPT_FACTS.\")\n\n# Verify your update\nverify = \"\"\"\n  SELECT meeting_date, customer_name, transcript_sentiment\n    FROM AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS\n   WHERE relative_path = ?\n\"\"\"\nrows = session.sql(verify, [relative_path]).collect()\nst.write(rows)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "380d841d-3082-4ad6-b3ce-94f8423b5b7d",
   "metadata": {
    "language": "python",
    "name": "CaptureKeyPhrases"
   },
   "outputs": [],
   "source": "# Join your already-sliced transcript dialogue chunks\ndialogue_text = \"\\n\\n\".join(dialogue_chunks)\n\n# Build a strict extraction prompt\nkp_prompt = f\"\"\"\nYou are a Snowflake analyst. From the dialogue below, extract ONLY the key phrases \nthat capture what the customer is asking for or highlighting.  \n\nReturn a JSON array of strings, nothing else.\n\nDialogue:\n<<<\n{dialogue_text}\n>>>\n\"\"\".strip()\n\n# Call Cortex Complete to get back JSON\nreply = XXX(                   # --> Use the Snowflake Complete function\n    model  = \"XXX\",   # use Anthropic model or Mistral Large model\n    prompt = kp_prompt\n).strip()\n\n# Parse JSON array, with a quick fallback to comma-split\ntry:\n    key_phrases = json.loads(reply)\n    if not isinstance(key_phrases, list):\n        raise ValueError(\"Expected a JSON list\")\n    key_phrases = [str(p).strip() for p in key_phrases if p]\nexcept Exception:\n    # fallback: strip out brackets and split on commas\n    key_phrases = [\n        p.strip().strip('\"').strip(\"'\")\n        for p in reply.strip(\"[]\").split(\",\")\n        if p.strip()\n    ]\n\n# ensure we have *some* list\nif not isinstance(key_phrases, list):\n    key_phrases = []\n\nst.success(f\"üîë Extracted {len(key_phrases)} key-phrases.\")\n\n# Persist into TRANSCRIPT_FACTS.key_phrases\nup = \"\"\"\n  UPDATE AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS\n     SET key_phrases = ?\n   WHERE relative_path = ?\n     AND customer_name = ?\n\"\"\"\nparams = [\n    key_phrases,\n    relative_path,\n    customer\n]\n\nsession.sql(up, params).collect()\nst.success(\"‚úÖ key_phrases saved to TRANSCRIPT_FACTS.\")\n\n# Verify & display the row\nverify = \"\"\"\n  SELECT meeting_date, customer_name, key_phrases\n    FROM AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS\n   WHERE relative_path = ?\n\"\"\"\nrow = session.sql(verify, [relative_path]).collect()\nst.write(row)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "87512b32-8e31-4350-8e7d-f4aa47790e08",
   "metadata": {
    "language": "python",
    "name": "AssessOpportunityRisk"
   },
   "outputs": [],
   "source": "# Join dialogue as before\ndialogue_text = \"\\n\\n\".join(dialogue_chunks)\n\n# Build prompt\nrisk_prompt = f\"\"\"\nYou are a Snowflake analyst assessing customer opportunity risk.  \nBased on the dialogue below, return a single numeric risk score between 0.0 (no risk) \nand 1.0 (very high risk), and nothing else.\n\nDialogue:\n<<<\n{dialogue_text}\n>>>\n\"\"\".strip()\n\n# Call LLM\nrisk_reply = XXX(              # --> Use the Snowflake Complete function\n    model  = \"XXX\",   # use Anthropic model or Mistral Large model\n    prompt = risk_prompt\n).strip()\n\n# Extract the first numeric token\nnum_match = re.search(r\"\\d+(?:\\.\\d+)?\", risk_reply)\nif not num_match:\n    raise ValueError(f\"Could not find a numeric risk score in: {risk_reply!r}\")\nrisk_score = float(num_match.group(0))\n\nst.success(f\"‚ö†Ô∏è Computed risk score: **{risk_score:.3f}**\")\n\n# Persist\nupd_sql = \"\"\"\n  UPDATE AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS\n     SET risk_scoring = ?\n   WHERE relative_path = ?\n     AND customer_name = ?\n\"\"\"\nsession.sql(upd_sql, [risk_score, relative_path, customer]).collect()\nst.success(\"‚úÖ risk_scoring saved to TRANSCRIPT_FACTS.\")\n\n# Verify\nverify_sql = \"\"\"\n  SELECT meeting_date, customer_name, risk_scoring\n    FROM AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS\n   WHERE relative_path = ?\n\"\"\"\nrow = session.sql(verify_sql, [relative_path]).collect()\nst.write(row)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c7199017-fce1-40d8-882d-2f0dd66364e6",
   "metadata": {
    "language": "sql",
    "name": "ViewEnhancedExtractions"
   },
   "outputs": [],
   "source": "SELECT * FROM AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "15f8d747-7033-4b46-ab75-2c2919e9ecfc",
   "metadata": {
    "name": "BatchRAGPipeline",
    "collapsed": false
   },
   "source": "### üöÄ Batch-Process All Discovery Transcripts\n\nIn the next step, we‚Äôll run a **single-cell pipeline** that iterates over all 6 customer transcripts and:\n\n1. **Retrieves** the top-*\\*k\\** chunks for each customer via your `TRANSCRIPTS_SEARCH_SERVICE`.  \n2. **Extracts** core meeting facts (`meeting_date`, `attendees`, `agenda`, `next_step`) using a JSON-strict LLM prompt.  \n3. **Computes** a **transcript_sentiment** score with `CORTEX.SENTIMENT()`.  \n4. **Derives** an array of **key_phrases** via the LLM to capture customer use-cases and pain points.  \n5. **Assesses** a numeric **risk_scoring** via the LLM, ranking opportunity health from 0.0 to 1.0.  \n6. **Persists** all fields back into your permanent `AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS` table in one go.\n\nBy combining retrieval, structured extraction, and LLM-driven enrichment in one cell, you‚Äôll instantly convert raw PDF transcripts into a **searchable, analytics-ready** dataset‚Äîno manual joins, no copy-pasting, no intermediate steps."
  },
  {
   "cell_type": "code",
   "id": "1dc37e70-b21f-4112-8b8d-28d1334a8d3d",
   "metadata": {
    "language": "python",
    "name": "BatchEnhancedRAGPipeline"
   },
   "outputs": [],
   "source": "# ‚îÄ‚îÄ Setup\nroot = Root(session)\nsvc  = root.databases[\"AICOLLEGE\"] \\\n           .schemas[\"PUBLIC\"] \\\n           .cortex_search_services[\"XXX\"]        # --> Use your Snowflake Cortex Search Service\n\n# pull all customer names\ncustomers = [\n    r[\"CUSTOMER_NAME\"]\n    for r in session\n            .table(\"CUSTOMER_INSIGHTS\")\n            .select(\"CUSTOMER_NAME\")\n            .distinct()\n            .collect()\n]\n\n# regexes for timestamp & floats\nts_re  = re.compile(r\"\\b\\d{1,2}:\\d{2}\\b\")\nnum_re = re.compile(r\"\\d+(?:\\.\\d+)?\")\n\nresults = []\n\n# Loop over every customer\nfor customer in customers:\n    question = (\n        f\"What was the meeting with {customer} about? \"\n        \"Who attended? When was the meeting? What is the next step?\"\n    )\n\n    # retrieve top‚Äêk context chunks for this customer\n    hits = svc.search(\n        query           = question,\n        columns         = [\"CUSTOMER_NAME\", \"CHUNK\", \"RELATIVE_PATH\"],\n        limit           = 10,\n        search_type     = \"hybrid\",\n        filters         = {\"CUSTOMER_NAME\":[customer]},\n        min_score       = 0.15,\n        max_chunk_chars = 400\n    ).results\n\n    if not hits:\n        continue\n\n    # Extract core facts via JSON prompt\n    context = \"\\n\\n---\\n\\n\".join(f\"[{i+1}] {h['CHUNK']}\" for i,h in enumerate(hits))\n    schema_prompt = f\"\"\"\nYou are a Snowflake analyst. From the context below extract ONLY these fields:\nmeeting_date, attendees, agenda, next_step.\nReturn a JSON object with exactly those keys. If a field is missing, use null.\n\nContext:\n<<<\n{context}\n>>>\n\"\"\".strip()\n\n    reply = Complete(model=\"mistral-large\", prompt=schema_prompt).strip()\n    try:\n        facts = json.loads(reply)\n    except Exception as e:\n        print(f\"‚ùå JSON parse failed for {customer}: {e!r}\")\n        continue\n\n    # add our identifiers\n    facts[\"customer_name\"] = customer\n    facts[\"relative_path\"] = hits[0][\"RELATIVE_PATH\"]\n\n    # Build transcript-only text\n    dialogue_chunks = []\n    for h in hits:\n        m = ts_re.search(h[\"CHUNK\"])\n        if m:\n            dialogue_chunks.append(h[\"CHUNK\"][m.start():])\n\n    transcript_text = (\n        \"\\n\\n\".join(dialogue_chunks)\n        if dialogue_chunks\n        else \"\\n\\n\".join(h[\"CHUNK\"] for h in hits)\n    )\n\n    # Compute transcript_sentiment\n    sent = session.sql(\n        \"SELECT SNOWFLAKE.XXX(?) AS S\", [transcript_text]      # --> Use the Snowflake Cortex Sentiment function\n    ).collect()[0][\"S\"]\n    facts[\"transcript_sentiment\"] = float(sent)\n\n    # Extract key_phrases via LLM\n    kp_prompt = f\"\"\"\nExtract the key phrases from the following customer dialogue.\nReturn a JSON array of phrases only, nothing else.\n\nDialogue:\n<<<\n{transcript_text}\n>>>\n\"\"\".strip()\n\n    kp_reply = XXX(model=\"XXX\", prompt=kp_prompt).strip()    # --> Use the Snowflake Cortex Complete function\n    try:\n        kp = json.loads(kp_reply)\n        if not isinstance(kp, list):\n            raise ValueError(\"not a list\")\n        key_phrases = [str(p).strip() for p in kp if p]\n    except:\n        # fallback to comma‚Äêsplit\n        key_phrases = [\n            p.strip().strip('\"')\n            for p in kp_reply.strip(\"[]\").split(\",\")\n            if p.strip()\n        ]\n    facts[\"key_phrases\"] = key_phrases\n\n    # Compute risk_scoring via LLM\n    risk_prompt = f\"\"\"\nBased on the following customer dialogue, return a single numeric risk score\nbetween 0.0 (no risk) and 1.0 (very high risk), and nothing else.\n\nDialogue:\n<<<\n{transcript_text}\n>>>\n\"\"\".strip()\n\n    risk_reply = XXX(model=\"XXX\", prompt=risk_prompt).strip()    # --> Use the Snowflake Cortex Complete function\n    m2 = num_re.search(risk_reply)\n    facts[\"risk_scoring\"] = float(m2.group(0)) if m2 else None\n\n    # Collect and move on\n    results.append(facts)\n\n# Write ALL results back in one shot\nif results:\n    df = session.create_dataframe(results)\n    df.write.save_as_table(\"XXX\", mode=\"overwrite\")        # --> Create a \"TRANSCRIPT_FACTS\" table for use with the semantic model\n    st.success(\"‚úÖ All transcripts (+ sentiment, key phrases, risk) saved to TRANSCRIPT_FACTS.\")\nelse:\n    st.warning(\"‚ö†Ô∏è No structured facts extracted; nothing written.\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c0e3d3f8-e3e6-4298-af51-5af843e6af31",
   "metadata": {
    "name": "AugmentSemanticModel",
    "collapsed": false
   },
   "source": "## üõ†Ô∏è Next Steps: Wire `TRANSCRIPT_FACTS` into Your Semantic Model & Search\n\nNow that you‚Äôve batch-loaded all **TRANSCRIPT_FACTS** (core meeting facts + sentiment, key-phrases, risk) into Snowflake, let‚Äôs extend your semantic model:\n\n1. **Add `TRANSCRIPT_FACTS` as a Logical Table**\n   - In your **data_agent_semantic_model.yaml**, declare a new table mapping to `AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS`.  \n   - Define its dimensions (`customer_name`, `meeting_date`, ‚Ä¶) and facts (`transcript_sentiment`, `risk_scoring`).  \n   - Set the primary key on (`relative_path`, `customer_name`, `meeting_date`).\n\n2. **Create Relationships**  \n   - **One-to-many**: link `CUSTOMER_INSIGHTS.meeting_id` ‚Üí `TRANSCRIPT_FACTS.relative_path` (or `customer_name` + `meeting_date`).  \n   - **One-to-one** (optional): connect `CUSTOMER_MEETING_OUTCOMES` ‚Üî `TRANSCRIPT_FACTS` on (`meeting_id` ‚Üí `relative_path`).  \n\n3. **Add Verified Queries**  \n   - Example: ‚ÄúWhich customers have **high transcript sentiment** (>0.5) but **low risk_scoring** (<0.3)?‚Äù  \n   - Example: ‚ÄúShow me **key phrases** by customer and **agenda** topics.‚Äù  \n   - Include these under `verified_queries` in your YAML so users can onboard quickly.\n\n4. **Enable Dynamic Literal Retrieval**  \n   - For any column with many possible values (e.g. `customer_name`, `snowflake_feature`, `industry`), configure a **Cortex Search Service** in Snowsight:  \n     ```sql\n     CREATE OR REPLACE CORTEX SEARCH SERVICE AICOLLEGE.PUBLIC.COLUMN_LOOKUP\n       ON <COLUMN_VALUE> \n       ATTRIBUTES <COLUMN_NAME>\n       WAREHOUSE = AICOLLEGE\n       EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\n       AS (SELECT DISTINCT <COLUMN_NAME> FROM <TABLE>);\n     ```  \n   - In the Snowsight UI, link that service under **Dynamic Literal Retrieval** so Analyst will look up filter values at runtime.\n\n5. **Validate & Deploy in Snowsight (No-Code)**  \n   - Use the Snowsight **Semantic Model Editor** to import your updated YAML.  \n   - Define the new table, relationships, and literal-lookup services.  \n   - Publish your changes.\n\n---\n\nüéâ  Once the semantic model is updated and deployed, return to **Slack** and try queries like:  \n> ‚Ä¢ ‚ÄúWhich customers requested a POC and have transcript_sentiment > 0.5?‚Äù  \n> ‚Ä¢ ‚ÄúList risk_scoring by region for all Gen AI discovery meetings.‚Äù  \n\nNow you‚Äôll seamlessly span **structured** + **unstructured** insights in one natural-language interface!"
  }
 ]
}