{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "uwskyl2ickjotzteiko6",
   "authorId": "154296475017",
   "authorName": "DSHAW_SFC",
   "authorEmail": "diana.shaw@snowflake.com",
   "sessionId": "5acc5de6-8e3b-4693-ba67-0103c9ec8cba",
   "lastEditTime": 1747153634566
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "Imports"
   },
   "source": "import warnings\nwarnings.filterwarnings(\"ignore\")\nimport streamlit as st\nimport pandas as pd\nimport json\nimport re\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import col\nfrom snowflake.snowpark import types as T\nfrom snowflake.snowpark import Row\nfrom snowflake.core import Root\nfrom snowflake.cortex import Complete\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "NotebookOverview",
    "collapsed": false,
    "codeCollapsed": true
   },
   "source": "# Define image in a stage and read the file\nimage=session.file.get_stream('@aicollege.public.transcripts/Phase2.jpg' , decompress=False).read() \n\n# Display the image\nst.image(image, width=400)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7dacb431-c209-4838-8354-1028ca344ffc",
   "metadata": {
    "language": "sql",
    "name": "ViewStage"
   },
   "outputs": [],
   "source": "-- View the list of files in the stage to ensure 6 are present\nLIST @AICOLLEGE.PUBLIC.TRANSCRIPTS PATTERN='.*\\.pdf$';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34d0755a-992b-4319-99d4-568bbc4b67f7",
   "metadata": {
    "language": "sql",
    "name": "ViewTranscript"
   },
   "outputs": [],
   "source": "-- View a transcript\nSELECT\n  SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n    '@AICOLLEGE.PUBLIC.TRANSCRIPTS',\n    'Acme Corp Services Discovery Meeting Transcript.pdf',\n    OBJECT_CONSTRUCT('mode','layout')\n  ) AS layout;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "61a854a7-cd27-4efa-a687-5b6a525898ad",
   "metadata": {
    "name": "PARSE_DOCUMENT",
    "collapsed": false
   },
   "source": "### 🧾 Extract PDF Transcript Content with `PARSE_DOCUMENT`\n\nWe’ll use the [`PARSE_DOCUMENT`](https://docs.snowflake.com/en/sql-reference/functions/parse_document-snowflake-cortex) function to extract the full contents of each transcript PDF.\n\nSnowflake has simplified working with unstructured documents by providing a Cortex AI SQL function that returns the extracted content as a structured JSON object — no external parsing tools required.\n\nThis is the **first step** in working with unstructured data, enabling:\n- RAG pipelines powered by Cortex Search\n- LLM workflows like summarization, translation, or classification\n- Structured output extraction from forms and contracts\n\n💡 `PARSE_DOCUMENT` supports two modes:\n- **OCR mode** → Best for clean text extraction from scanned documents  \n- **LAYOUT mode** → Best for preserving structure like tables, headers, and sections (used here)\n\nWe’ll store the extracted content along with customer name and file path in a `PARSED_TRANSCRIPTS` table for future use."
  },
  {
   "cell_type": "code",
   "id": "d1860fdc-701c-44b3-83c8-4b0bfc62ee56",
   "metadata": {
    "language": "sql",
    "name": "ParseTranscripts"
   },
   "outputs": [],
   "source": "-- Create the parsed_transcripts table\nCREATE OR REPLACE TABLE PARSED_TRANSCRIPTS (\n    CUSTOMER_NAME  STRING,                      -- e.g. “Acme Corp Services”\n    RELATIVE_PATH  STRING PRIMARY KEY,          -- full path in the stage\n    RAW_TEXT       VARIANT,                     -- PDF text\n    LOADED_AT      TIMESTAMP_TZ DEFAULT CURRENT_TIMESTAMP()\n);\n\n-- Process files one at a time to avoid stage scanning issues\nINSERT INTO PARSED_TRANSCRIPTS (CUSTOMER_NAME, RELATIVE_PATH, RAW_TEXT)\nSELECT \n    REGEXP_REPLACE(RELATIVE_PATH,' Discovery Meeting Transcript\\\\.pdf$','') AS CUSTOMER_NAME,\n    RELATIVE_PATH,\n    SNOWFLAKE.XXX(      --> Use Snowflake's Parse Document function\n        '@AICOLLEGE.PUBLIC.TRANSCRIPTS',\n        RELATIVE_PATH,\n        OBJECT_CONSTRUCT('mode','XXX') --> Use layout mode\n    ) AS RAW_TEXT\nFROM DIRECTORY('@AICOLLEGE.PUBLIC.TRANSCRIPTS') f\nWHERE\n  RELATIVE_PATH LIKE '%Discovery Meeting Transcript.pdf'\n  AND RELATIVE_PATH NOT IN (SELECT RELATIVE_PATH FROM PARSED_TRANSCRIPTS);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "75324729-c49d-47dd-af2e-965c81a04420",
   "metadata": {
    "language": "sql",
    "name": "SeeParsedText"
   },
   "outputs": [],
   "source": "-- Validate PARSED_TRANSCRIPTS was created correctly\nSELECT * FROM PARSED_TRANSCRIPTS LIMIT 5;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6b8251a8-8380-4a86-a8b2-d13ea6cef003",
   "metadata": {
    "name": "SPLIT_TEXT_RECURSIVE_CHARACTER",
    "collapsed": false
   },
   "source": "### ✂️ Chunk Transcript Text with SPLIT_TEXT_RECURSIVE_CHARACTER\nOnce we’ve extracted the full layout of each transcript, the next step is to split the long text into smaller overlapping chunks. This is a critical step before embedding or indexing for semantic search.\n\nWe’ll use the [`SPLIT_TEXT_RECURSIVE_CHARACTER`](https://docs.snowflake.com/en/sql-reference/functions/split_text_recursive_character-snowflake-cortex) function, which is designed to split long documents intelligently based on a preferred delimiter (e.g., paragraph breaks).\n\nThis prepares the data for downstream use with:\n- Cortex Search indexing\n- Embedding generation\n- Summarization or classification with LLMs\n\n💡 We’re using:\n- A chunk size of **800 characters**\n- An overlap of **150 characters**\n- A preferred break on \"\\n\\n\" to preserve paragraph structure"
  },
  {
   "cell_type": "code",
   "id": "be605bb5-63d0-43b1-88ac-6b832aae107f",
   "metadata": {
    "language": "sql",
    "name": "ChunkTranscripts"
   },
   "outputs": [],
   "source": "-- Create chunks of the parsed transcripts\nCREATE OR REPLACE TABLE TRANSCRIPT_CHUNKS (\n    CUSTOMER_NAME  STRING,\n    RELATIVE_PATH  STRING,\n    CHUNK          STRING\n);\n\n-- Insert chunked content using SPLIT_TEXT_RECURSIVE_CHARACTER\nINSERT INTO TRANSCRIPT_CHUNKS (CUSTOMER_NAME, RELATIVE_PATH, CHUNK)\nWITH text_chunks AS (\n    SELECT\n        CUSTOMER_NAME,\n        RELATIVE_PATH,\n        SNOWFLAKE.XXX(  --> Use Snowflake's Split Text Recursive function\n            RAW_TEXT:content::STRING,  -- extract string content\n            'markdown',                -- tokenizer\n            800,                       -- chunk size\n            100,                       -- overlap\n            ARRAY_CONSTRUCT('\\n\\n')    -- preferred break\n        ) AS CHUNKS\n    FROM PARSED_TRANSCRIPTS\n    WHERE RAW_TEXT:content IS NOT NULL -- optional safety check\n)\nSELECT\n    CUSTOMER_NAME,\n    RELATIVE_PATH,\n    chunk.value::STRING AS XXX  --> Save chunks of parsed transcripts as \"CHUNK\"\nFROM text_chunks,\nLATERAL FLATTEN(input => CHUNKS) AS chunk;  --> Flatten the chuncked output",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e19e0835-5c5d-4579-856a-858fdae729a5",
   "metadata": {
    "language": "sql",
    "name": "SeeChunkedText"
   },
   "outputs": [],
   "source": "-- Validate TRANSCRIPT_CHUNKS was created correctly\nSELECT * FROM TRANSCRIPT_CHUNKS LIMIT 5;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "249d037c-8049-4b3e-9a25-cd8b3c92077d",
   "metadata": {
    "name": "CORTEX_SEARCH_SERVICES",
    "collapsed": false
   },
   "source": "### 🔍 Make Your Transcripts Searchable with Cortex Search\n\nNow that we've extracted and chunked the content of each transcript, the next step is to make it **searchable** using Snowflake's fully managed retrieval system: **Cortex Search**.\n\n[`CORTEX SEARCH SERVICE`](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-search/cortex-search-overview) enables **hybrid retrieval** — combining vector search (semantic meaning) with keyword search (lexical matching) — to deliver **highly accurate results** out of the box.\n\nThis capability is a key enabler for Retrieval-Augmented Generation (RAG), allowing you to:\n- Ask contextual questions over your documents using SQL or LLM interfaces\n- Power chatbots and copilots with document-grounded responses\n- Serve precise and explainable enterprise search across large volumes of unstructured content\n\n💡 Powered by `snowflake-arctic-embed-l-v2.0`, a state-of-the-art embedding model optimized for enterprise search.\n\nOnce the search service is defined, you can start querying it with natural language — directly inside Snowflake — without needing external pipelines, vector DBs, or hosting infrastructure."
  },
  {
   "cell_type": "code",
   "id": "a8e9af90-1d61-4c26-bd40-d24e2abdb14b",
   "metadata": {
    "language": "sql",
    "name": "CreateCortexSearchService"
   },
   "outputs": [],
   "source": "-- Create a Cortex Search Service over your chunked transcripts table\nCREATE OR REPLACE XXX AICOLLEGE.PUBLIC.TRANSCRIPTS_SEARCH_SERVICE  --> Create \"TRANSCRIPTS_SEARCH_SERVICE\" via Snowflake's Cortex Search Service hybrid retrieval function\n    ON CHUNK\n    ATTRIBUTES CUSTOMER_NAME, RELATIVE_PATH\n    WAREHOUSE = AICOLLEGE\n    TARGET_LAG = '365 days'\n    EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'  -- Optional: Select your desired embedding model\n    AS (\n    SELECT\n        CUSTOMER_NAME,\n        RELATIVE_PATH,\n        CHUNK::VARCHAR AS CHUNK,\n    FROM TRANSCRIPT_CHUNKS);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6eb4d5ff-3d60-4d65-bd50-b76a47db2df0",
   "metadata": {
    "name": "SimpleRAGPipeline",
    "collapsed": false
   },
   "source": "### Test your service — Baseline **“one-shot” RAG** (even with a top-tier model)\n\nSelect a customer and then run the *BaselineRAGPipeline* cell to perform the **simplest possible Retrieval-Augmented Generation flow**:\n\n1. **Retrieve** the **first** transcript chunk that semantically matches the user’s question.  \n2. **Generate** an answer by stuffing that single chunk into the prompt of a **state-of-the-art model** (*`claude-3-5-sonnet`* or *`mistral-large2`*).\n\n---\n\n> Even with a premium LLM you’ll notice it can only repeat whatever facts happen to live in that lone chunk.  \n> It frequently replies with **“Based on the limited context provided …”** because:\n>\n> * The relevant details might sit in a **different** chunk.  \n> * We pass **no metadata** (e.g., speaker tags, meeting header) to help the model reason.  \n> * We don’t ask it to **extract** or **structure** anything.\n>\n> You’ll address all of these gaps in the next steps."
  },
  {
   "cell_type": "code",
   "id": "def68695-2e65-405e-9a89-8bea54f3193d",
   "metadata": {
    "language": "python",
    "name": "SelectCustomer",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Run this and then select a distinct customer from the dropdown list\ncustomers = [r[\"CUSTOMER_NAME\"] for r in session.table(\"CUSTOMER_INSIGHTS\").select(\"CUSTOMER_NAME\").distinct().collect()]\n\ncustomers = sorted(customers)\n\nselected = st.selectbox(\n    \"Run this cell, then select a customer to analyze\",\n    options=customers,\n    key=\"selected_customer\"\n)\n\nst.success(f\"Chosen customer: {st.session_state.selected_customer}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "056d319b-65aa-4503-92cb-50efd579adcc",
   "metadata": {
    "language": "python",
    "name": "BaselineRAGPipeline"
   },
   "outputs": [],
   "source": "# Create service handle\nroot = Root(get_active_session())\ncustomer = st.session_state.selected_customer\nsvc  = root.databases[\"AICOLLEGE\"]\\\n           .schemas[\"PUBLIC\"]\\\n           .cortex_search_services[\"XXX\"]  # --> Use your Snowflake Cortex Search Service\n\n# Specify user question\nquestion = f\"\"\"\n            What was the meeting with {customer} about? \n            Who attended? When was the meeting? What is the next step?\n            \"\"\".strip()\n\n# Get naive retrieval – grab the very first hit (could be just the title page)\nhit = svc.search(\n        query            = question,\n        columns          = [\"CUSTOMER_NAME\", \"CHUNK\", \"RELATIVE_PATH\"],\n        limit            = 5,                # look at a few chunks\n        search_type      = \"embed\"           # basic semantic search\n     ).results[0]\n\nst.info(f\"**File:** {hit['RELATIVE_PATH']}\\n\\n{hit['CHUNK']}\")\n\n# Single-pass generation\nresponse = Complete(\n             model  = \"XXX\",  # --> use Anthropic model or Mistral Large model\n             prompt = (\n                 f\"{question}\\n\\n\"\n                 \"Answer **only** using the context below:\\n\"\n                 \"-----\\n\"\n                 f\"{hit['CHUNK']}\\n\"\n                 \"-----\"\n             )\n           ).strip()\n\nst.info(f\"**LLM Response:**\\n\\n{response}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "478861a6-d49e-4666-9c78-4b859d76dea7",
   "metadata": {
    "name": "EnhancedRAGPipeline",
    "collapsed": false
   },
   "source": "### 🧨 From “one-shot” to 🎯 **Production-ready**: the 3-Step Enhanced RAG Pipeline  \n\n| Step | What happens | Why it matters |\n|------|--------------|----------------|\n| **1&nbsp;·&nbsp;Smart Retrieval** *(hybrid search + customer filter + top-k)* | We ask `TRANSCRIPTS_SEARCH_SERVICE` for the **five** best chunks that match the user’s question **and** the selected customer.  Hybrid mode blends dense-vector recall *and* keyword precision; score / size filters keep the text relevant and compact. | Pulls **all** salient passages (not just the first hit) while avoiding noise. Higher recall ⇒ fewer “missing-context” answers and less hallucination. |\n| **2&nbsp;·&nbsp;Schema-Driven Extraction** *(LLM returns strict JSON)* | A purpose-built prompt instructs the LLM to emit **only** `meeting_date, attendees, agenda, next_step` as JSON.  The prompt includes the concatenated chunks with clear separators. | Guarantees a predictable shape you can trust in code.  Eliminates manual parsing.  Minimises off-topic text & hallucinations. |\n| **3&nbsp;·&nbsp;Persist Results** *(Snowpark DataFrame → `TRANSCRIPT_FACTS`)* | The JSON is converted to a Snowpark DF and saved as a table.  Each row is enriched with `customer_name` & `relative_path` so you know exactly where it came from. | Makes the extracted facts **queryable in SQL** immediately. We will JOIN it with your Phase 1 tables or add it to your semantic model.|\n\n---\n\n### Why this 3-Step Enhanced RAG Pipeline beats the “one-shot” Baseline\n\n* **More context ≠ more noise** – Hybrid retrieval surfaces the right chunks, not just the first one.  \n* **LLM as parser**, not essayist – A tight schema prompt forces the model to structure data instead of rambling.  \n* **SQL-ready output** – Persisting to a temp table lets analysts (or Cortex Analyst) treat unstructured insights just like any other table—no copy-paste required.\n\n> **Bottom line:** this 3-step pipeline turns messy PDF transcripts into reliable, query-ready facts for efficient and accurate results."
  },
  {
   "cell_type": "code",
   "id": "dc238a9e-4ce1-4f64-9408-15fa35841005",
   "metadata": {
    "language": "python",
    "name": "HybridSearch"
   },
   "outputs": [],
   "source": "# Build better retrieval (chunk filter + hybrid search) to retrieve top-k chunks for Acme Corp\nhits = svc.search(\n    query           = question,\n    columns         = [\"CUSTOMER_NAME\", \"CHUNK\", \"RELATIVE_PATH\"],\n    limit           = 4,                 # grab more context\n    search_type     = \"hybrid\",           # embedding + keyword\n    filters         = {\"CUSTOMER_NAME\": [customer]},  # force-match\n    min_score       = 0.15,              # drop low-relevance chunks\n    max_chunk_chars = 400                # trim huge blobs\n).results\n\n# join them into one context string\ncontext = \"\\n\\n---\\n\\n\".join(f\"[{i+1}] {h['CHUNK']}\" for i,h in enumerate(hits))\nst.info(context)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d88ac95-2e8d-45ca-b475-c270b716cc27",
   "metadata": {
    "language": "python",
    "name": "StructuredJSONExtraction"
   },
   "outputs": [],
   "source": "# Build structured extraction prompt to return strict JSON so it's easier to load into a table\nschema_prompt = f\"\"\"\nYou are a Snowflake analyst. From the context below extract ONLY these fields:\nmeeting_date, attendees, agenda, next_step.\nReturn a JSON object with exactly those keys. If a field is missing, use null.\n\nContext:\n<<<\n{context}\n>>>\n\"\"\".strip()\n\n# call the LLM (no response_format kwarg on this API)\nreply_text = XXX(              # --> Use the Snowflake Complete function\n    model  = \"claude-3-5-sonnet\",   # use Anthropic model or Mistral Large model\n    prompt = schema_prompt\n).strip()\n\n# parse the JSON\nfacts = json.loads(reply_text)\n\n# enrich\nfacts[\"customer_name\"]  = customer\nfacts[\"relative_path\"]  = hits[0][\"RELATIVE_PATH\"]\n\n# show it\nst.json(facts)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab8fed0e-593a-432b-914b-103c50cc1d23",
   "metadata": {
    "language": "python",
    "name": "PersistExtractText"
   },
   "outputs": [],
   "source": "# Persist extracted facts into a table with explicit schema\n# Normalise every field that must be STRING\ndef to_text(v):\n    \"\"\"Convert list/tuple/dict/None to a readable string for Snowflake.\"\"\"\n    if v is None:\n        return \"\"\n    if isinstance(v, (list, tuple)):\n        # Flatten nested lists first\n        flat = []\n        for x in v:\n            if isinstance(x, (list, tuple)):\n                flat.extend(x)\n            else:\n                flat.append(x)\n        return \"; \".join(str(x) for x in flat)\n    if isinstance(v, dict):\n        return json.dumps(v, ensure_ascii=False)\n    # already scalar (str, int, float, etc.)\n    return str(v)\n\nfacts[\"agenda\"]     = to_text(facts.get(\"agenda\"))\nfacts[\"next_step\"]  = to_text(facts.get(\"next_step\"))\n\n# Ensure attendees is a clean array of strings\natt = facts.get(\"attendees\")\nif isinstance(att, dict):\n    facts[\"attendees\"] = [v for _, v in sorted(att.items(), key=lambda kv: int(kv[0]))]\nelif isinstance(att, (list, tuple)):\n    facts[\"attendees\"] = [str(a) for a in att]          # coerce to str\nelse:\n    facts[\"attendees\"] = [str(att)] if att is not None else []\n\n# Define the Snowpark schema\nfact_schema = T.StructType([\n    T.StructField(\"meeting_date\",    T.StringType()),\n    T.StructField(\"attendees\",       T.ArrayType(T.StringType())),\n    T.StructField(\"agenda\",          T.StringType()),\n    T.StructField(\"next_step\",       T.StringType()),\n    T.StructField(\"customer_name\",   T.StringType()),\n    T.StructField(\"relative_path\",   T.StringType())\n])\n\n# Create the DataFrame\nsnow_df = session.create_dataframe([facts], schema=fact_schema)\n\n# Display in Streamlit\nst.markdown(\"**Structured meeting facts:**\")\nst.dataframe(snow_df.to_pandas())\n\n# Save as table\nsnow_df.write.save_as_table(\"XXX\", mode=\"overwrite\", table_type=\"XXX\")  # --> Create a TEMPORARY table called \"TRANSCRIPT_FACTS\"\n\nst.success(\"✅ Facts saved to table `TRANSCRIPT_FACTS`\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1a36b9e5-c37c-4edf-b684-14f23870cb03",
   "metadata": {
    "name": "MoreUnstructuredExtractEnahancements",
    "collapsed": false
   },
   "source": "### Enrich Your Transcript Facts with Sentiment, Key Phrases & Risk Scoring\n\nBefore you run the batch pipeline, let’s add three powerful new columns to your `TRANSCRIPT_FACTS` table:\n\n1. **`TRANSCRIPT_SENTIMENT`** – A 0.0–1.0 score capturing the overall tone of the **customer’s dialogue** (timestamps only).  \n2. **`KEY_PHRASES`** – An array of distilled phrases representing the customer’s top **use cases**, **pain points**, or **requirements**.  \n3. **`RISK_SCORING`** – A 0.0–1.0 numeric indicator of **deal risk**, as assessed by the LLM based on customer language.\n\nThese enrichments let you:\n\n- **Track sentiment** by customer or region to spot relationship health.  \n- **Filter or group** by specific key phrases (e.g. “Snowpark POC”, “data governance”).  \n- **Prioritize follow-ups** by sorting deals by risk score. "
  },
  {
   "cell_type": "code",
   "id": "d4c2ef7d-c6e8-41a8-8843-7246735eaa06",
   "metadata": {
    "language": "sql",
    "name": "MoreEnhancements"
   },
   "outputs": [],
   "source": "ALTER TABLE TRANSCRIPT_FACTS \nADD (\n    TRANSCRIPT_SENTIMENT DOUBLE,\n    KEY_PHRASES ARRAY,\n    RISK_SCORING FLOAT\n);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6924f063-a309-43f1-9fdb-3dfc2ab11a28",
   "metadata": {
    "language": "python",
    "name": "TranscriptSentiment"
   },
   "outputs": [],
   "source": "# Unpack identifiers (from your JSON‐extraction cell) \ncustomer      = facts[\"customer_name\"]\nrelative_path = facts[\"relative_path\"]\nmeeting_date  = facts.get(\"meeting_date\")   # might be None\n\n# Regex to find a “hh:mm” timestamp in the chunk\nts_re = re.compile(r\"\\b\\d{1,2}:\\d{2}\\b\")\n\ndialogue_chunks = []\nfor h in hits:  \n    text = h[\"CHUNK\"]\n    m = ts_re.search(text)\n    if m:\n        # grab from the first timestamp through the end\n        dialogue_chunks.append(text[m.start():])\n    # else: skip this chunk entirely (it was probably header/overview)\n\n# Fallback if *none* of your hits had timestamps\nif dialogue_chunks:\n    transcript_text = \"\\n\\n\".join(dialogue_chunks)\nelse:\n    # last resort: join all hits (you will see header, but at least you get something)\n    transcript_text = \"\\n\\n\".join(h[\"CHUNK\"] for h in hits)\n\nst.info(f\"✂️ Snippet of transcript used for sentiment (only first 500 chars shown):\\n\\n{transcript_text[:500]}…\")\n\n# Score via a single CORTEX.SENTIMENT() call\nsent_score = session.sql(\n    \"SELECT SNOWFLAKE.XXX(?) AS S\",   # --> Use the Snowflake Cortex Sentiment function\n    [transcript_text]\n).collect()[0][\"S\"]\n\n# Write back into TRANSCRIPT_FACTS.transcript_sentiment\nif meeting_date is not None:\n    upd = \"\"\"\n      UPDATE AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS\n         SET transcript_sentiment = ?\n       WHERE relative_path = ?\n         AND customer_name = ?\n         AND meeting_date  = ?\n    \"\"\"\n    params = [float(sent_score), relative_path, customer, meeting_date]\nelse:\n    upd = \"\"\"\n      UPDATE AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS\n         SET transcript_sentiment = ?\n       WHERE relative_path = ?\n         AND customer_name = ?\n    \"\"\"\n    params = [float(sent_score), relative_path, customer]\n\nsession.sql(upd, params).collect()\nst.success(f\"✅ transcript_sentiment = {sent_score:.3f} saved to TRANSCRIPT_FACTS.\")\n\n# Verify your update\nverify = \"\"\"\n  SELECT meeting_date, customer_name, transcript_sentiment\n    FROM AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS\n   WHERE relative_path = ?\n\"\"\"\nrows = session.sql(verify, [relative_path]).collect()\nst.write(rows)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "380d841d-3082-4ad6-b3ce-94f8423b5b7d",
   "metadata": {
    "language": "python",
    "name": "CaptureKeyPhrases"
   },
   "outputs": [],
   "source": "# Join your already-sliced transcript dialogue chunks\ndialogue_text = \"\\n\\n\".join(dialogue_chunks)\n\n# Build a strict extraction prompt\nkp_prompt = f\"\"\"\nYou are a Snowflake analyst. From the dialogue below, extract ONLY the key phrases \nthat capture what the customer is asking for or highlighting.  \n\nReturn a JSON array of strings, nothing else.\n\nDialogue:\n<<<\n{dialogue_text}\n>>>\n\"\"\".strip()\n\n# Call Cortex Complete to get back JSON\nreply = XXX(                   # --> Use the Snowflake Complete function\n    model  = \"XXX\",   # use Anthropic model or Mistral Large model\n    prompt = kp_prompt\n).strip()\n\n# Parse JSON array, with a quick fallback to comma-split\ntry:\n    key_phrases = json.loads(reply)\n    if not isinstance(key_phrases, list):\n        raise ValueError(\"Expected a JSON list\")\n    key_phrases = [str(p).strip() for p in key_phrases if p]\nexcept Exception:\n    # fallback: strip out brackets and split on commas\n    key_phrases = [\n        p.strip().strip('\"').strip(\"'\")\n        for p in reply.strip(\"[]\").split(\",\")\n        if p.strip()\n    ]\n\n# ensure we have *some* list\nif not isinstance(key_phrases, list):\n    key_phrases = []\n\nst.success(f\"🔑 Extracted {len(key_phrases)} key-phrases.\")\n\n# Persist into TRANSCRIPT_FACTS.key_phrases\nup = \"\"\"\n  UPDATE AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS\n     SET key_phrases = ?\n   WHERE relative_path = ?\n     AND customer_name = ?\n\"\"\"\nparams = [\n    key_phrases,\n    relative_path,\n    customer\n]\n\nsession.sql(up, params).collect()\nst.success(\"✅ key_phrases saved to TRANSCRIPT_FACTS.\")\n\n# Verify & display the row\nverify = \"\"\"\n  SELECT meeting_date, customer_name, key_phrases\n    FROM AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS\n   WHERE relative_path = ?\n\"\"\"\nrow = session.sql(verify, [relative_path]).collect()\nst.write(row)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "87512b32-8e31-4350-8e7d-f4aa47790e08",
   "metadata": {
    "language": "python",
    "name": "AssessOpportunityRisk"
   },
   "outputs": [],
   "source": "# Join dialogue as before\ndialogue_text = \"\\n\\n\".join(dialogue_chunks)\n\n# Build prompt\nrisk_prompt = f\"\"\"\nYou are a Snowflake analyst assessing customer opportunity risk.  \nBased on the dialogue below, return a single numeric risk score between 0.0 (no risk) \nand 1.0 (very high risk), and nothing else.\n\nDialogue:\n<<<\n{dialogue_text}\n>>>\n\"\"\".strip()\n\n# Call LLM\nrisk_reply = XXX(              # --> Use the Snowflake Complete function\n    model  = \"XXX\",   # use Anthropic model or Mistral Large model\n    prompt = risk_prompt\n).strip()\n\n# Extract the first numeric token\nnum_match = re.search(r\"\\d+(?:\\.\\d+)?\", risk_reply)\nif not num_match:\n    raise ValueError(f\"Could not find a numeric risk score in: {risk_reply!r}\")\nrisk_score = float(num_match.group(0))\n\nst.success(f\"⚠️ Computed risk score: **{risk_score:.3f}**\")\n\n# Persist\nupd_sql = \"\"\"\n  UPDATE AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS\n     SET risk_scoring = ?\n   WHERE relative_path = ?\n     AND customer_name = ?\n\"\"\"\nsession.sql(upd_sql, [risk_score, relative_path, customer]).collect()\nst.success(\"✅ risk_scoring saved to TRANSCRIPT_FACTS.\")\n\n# Verify\nverify_sql = \"\"\"\n  SELECT meeting_date, customer_name, risk_scoring\n    FROM AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS\n   WHERE relative_path = ?\n\"\"\"\nrow = session.sql(verify_sql, [relative_path]).collect()\nst.write(row)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c7199017-fce1-40d8-882d-2f0dd66364e6",
   "metadata": {
    "language": "sql",
    "name": "ViewEnhancedExtractions"
   },
   "outputs": [],
   "source": "SELECT * FROM AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "15f8d747-7033-4b46-ab75-2c2919e9ecfc",
   "metadata": {
    "name": "BatchRAGPipeline",
    "collapsed": false
   },
   "source": "### 🚀 Batch-Process All Discovery Transcripts\n\nIn the next step, we’ll run a **single-cell pipeline** that iterates over all 6 customer transcripts and:\n\n1. **Retrieves** the top-*\\*k\\** chunks for each customer via your `TRANSCRIPTS_SEARCH_SERVICE`.  \n2. **Extracts** core meeting facts (`meeting_date`, `attendees`, `agenda`, `next_step`) using a JSON-strict LLM prompt.  \n3. **Computes** a **transcript_sentiment** score with `CORTEX.SENTIMENT()`.  \n4. **Derives** an array of **key_phrases** via the LLM to capture customer use-cases and pain points.  \n5. **Assesses** a numeric **risk_scoring** via the LLM, ranking opportunity health from 0.0 to 1.0.  \n6. **Persists** all fields back into your permanent `AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS` table in one go.\n\nBy combining retrieval, structured extraction, and LLM-driven enrichment in one cell, you’ll instantly convert raw PDF transcripts into a **searchable, analytics-ready** dataset—no manual joins, no copy-pasting, no intermediate steps."
  },
  {
   "cell_type": "code",
   "id": "1dc37e70-b21f-4112-8b8d-28d1334a8d3d",
   "metadata": {
    "language": "python",
    "name": "BatchEnhancedRAGPipeline"
   },
   "outputs": [],
   "source": "# ── Setup\nroot = Root(session)\nsvc  = root.databases[\"AICOLLEGE\"] \\\n           .schemas[\"PUBLIC\"] \\\n           .cortex_search_services[\"XXX\"]        # --> Use your Snowflake Cortex Search Service\n\n# pull all customer names\ncustomers = [\n    r[\"CUSTOMER_NAME\"]\n    for r in session\n            .table(\"CUSTOMER_INSIGHTS\")\n            .select(\"CUSTOMER_NAME\")\n            .distinct()\n            .collect()\n]\n\n# regexes for timestamp & floats\nts_re  = re.compile(r\"\\b\\d{1,2}:\\d{2}\\b\")\nnum_re = re.compile(r\"\\d+(?:\\.\\d+)?\")\n\nresults = []\n\n# Loop over every customer\nfor customer in customers:\n    question = (\n        f\"What was the meeting with {customer} about? \"\n        \"Who attended? When was the meeting? What is the next step?\"\n    )\n\n    # retrieve top‐k context chunks for this customer\n    hits = svc.search(\n        query           = question,\n        columns         = [\"CUSTOMER_NAME\", \"CHUNK\", \"RELATIVE_PATH\"],\n        limit           = 10,\n        search_type     = \"hybrid\",\n        filters         = {\"CUSTOMER_NAME\":[customer]},\n        min_score       = 0.15,\n        max_chunk_chars = 400\n    ).results\n\n    if not hits:\n        continue\n\n    # Extract core facts via JSON prompt\n    context = \"\\n\\n---\\n\\n\".join(f\"[{i+1}] {h['CHUNK']}\" for i,h in enumerate(hits))\n    schema_prompt = f\"\"\"\nYou are a Snowflake analyst. From the context below extract ONLY these fields:\nmeeting_date, attendees, agenda, next_step.\nReturn a JSON object with exactly those keys. If a field is missing, use null.\n\nContext:\n<<<\n{context}\n>>>\n\"\"\".strip()\n\n    reply = Complete(model=\"mistral-large\", prompt=schema_prompt).strip()\n    try:\n        facts = json.loads(reply)\n    except Exception as e:\n        print(f\"❌ JSON parse failed for {customer}: {e!r}\")\n        continue\n\n    # add our identifiers\n    facts[\"customer_name\"] = customer\n    facts[\"relative_path\"] = hits[0][\"RELATIVE_PATH\"]\n\n    # Build transcript-only text\n    dialogue_chunks = []\n    for h in hits:\n        m = ts_re.search(h[\"CHUNK\"])\n        if m:\n            dialogue_chunks.append(h[\"CHUNK\"][m.start():])\n\n    transcript_text = (\n        \"\\n\\n\".join(dialogue_chunks)\n        if dialogue_chunks\n        else \"\\n\\n\".join(h[\"CHUNK\"] for h in hits)\n    )\n\n    # Compute transcript_sentiment\n    sent = session.sql(\n        \"SELECT SNOWFLAKE.XXX(?) AS S\", [transcript_text]      # --> Use the Snowflake Cortex Sentiment function\n    ).collect()[0][\"S\"]\n    facts[\"transcript_sentiment\"] = float(sent)\n\n    # Extract key_phrases via LLM\n    kp_prompt = f\"\"\"\nExtract the key phrases from the following customer dialogue.\nReturn a JSON array of phrases only, nothing else.\n\nDialogue:\n<<<\n{transcript_text}\n>>>\n\"\"\".strip()\n\n    kp_reply = XXX(model=\"XXX\", prompt=kp_prompt).strip()    # --> Use the Snowflake Cortex Complete function\n    try:\n        kp = json.loads(kp_reply)\n        if not isinstance(kp, list):\n            raise ValueError(\"not a list\")\n        key_phrases = [str(p).strip() for p in kp if p]\n    except:\n        # fallback to comma‐split\n        key_phrases = [\n            p.strip().strip('\"')\n            for p in kp_reply.strip(\"[]\").split(\",\")\n            if p.strip()\n        ]\n    facts[\"key_phrases\"] = key_phrases\n\n    # Compute risk_scoring via LLM\n    risk_prompt = f\"\"\"\nBased on the following customer dialogue, return a single numeric risk score\nbetween 0.0 (no risk) and 1.0 (very high risk), and nothing else.\n\nDialogue:\n<<<\n{transcript_text}\n>>>\n\"\"\".strip()\n\n    risk_reply = XXX(model=\"XXX\", prompt=risk_prompt).strip()    # --> Use the Snowflake Cortex Complete function\n    m2 = num_re.search(risk_reply)\n    facts[\"risk_scoring\"] = float(m2.group(0)) if m2 else None\n\n    # Collect and move on\n    results.append(facts)\n\n# Write ALL results back in one shot\nif results:\n    df = session.create_dataframe(results)\n    df.write.save_as_table(\"XXX\", mode=\"overwrite\")        # --> Create a \"TRANSCRIPT_FACTS\" table for use with the semantic model\n    st.success(\"✅ All transcripts (+ sentiment, key phrases, risk) saved to TRANSCRIPT_FACTS.\")\nelse:\n    st.warning(\"⚠️ No structured facts extracted; nothing written.\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c0e3d3f8-e3e6-4298-af51-5af843e6af31",
   "metadata": {
    "name": "AugmentSemanticModel",
    "collapsed": false
   },
   "source": "## 🛠️ Next Steps: Wire `TRANSCRIPT_FACTS` into Your Semantic Model & Search\n\nNow that you’ve batch-loaded all **TRANSCRIPT_FACTS** (core meeting facts + sentiment, key-phrases, risk) into Snowflake, let’s extend your semantic model:\n\n1. **Add `TRANSCRIPT_FACTS` as a Logical Table**\n   - In your **data_agent_semantic_model.yaml**, declare a new table mapping to `AICOLLEGE.PUBLIC.TRANSCRIPT_FACTS`.  \n   - Define its dimensions (`customer_name`, `meeting_date`, …) and facts (`transcript_sentiment`, `risk_scoring`).  \n   - Set the primary key on (`relative_path`, `customer_name`, `meeting_date`).\n\n2. **Create Relationships**  \n   - **One-to-many**: link `CUSTOMER_INSIGHTS.meeting_id` → `TRANSCRIPT_FACTS.relative_path` (or `customer_name` + `meeting_date`).  \n   - **One-to-one** (optional): connect `CUSTOMER_MEETING_OUTCOMES` ↔ `TRANSCRIPT_FACTS` on (`meeting_id` → `relative_path`).  \n\n3. **Add Verified Queries**  \n   - Example: “Which customers have **high transcript sentiment** (>0.5) but **low risk_scoring** (<0.3)?”  \n   - Example: “Show me **key phrases** by customer and **agenda** topics.”  \n   - Include these under `verified_queries` in your YAML so users can onboard quickly.\n\n4. **Enable Dynamic Literal Retrieval**  \n   - For any column with many possible values (e.g. `customer_name`, `snowflake_feature`, `industry`), configure a **Cortex Search Service** in Snowsight:  \n     ```sql\n     CREATE OR REPLACE CORTEX SEARCH SERVICE AICOLLEGE.PUBLIC.COLUMN_LOOKUP\n       ON <COLUMN_VALUE> \n       ATTRIBUTES <COLUMN_NAME>\n       WAREHOUSE = AICOLLEGE\n       EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\n       AS (SELECT DISTINCT <COLUMN_NAME> FROM <TABLE>);\n     ```  \n   - In the Snowsight UI, link that service under **Dynamic Literal Retrieval** so Analyst will look up filter values at runtime.\n\n5. **Validate & Deploy in Snowsight (No-Code)**  \n   - Use the Snowsight **Semantic Model Editor** to import your updated YAML.  \n   - Define the new table, relationships, and literal-lookup services.  \n   - Publish your changes.\n\n---\n\n🎉  Once the semantic model is updated and deployed, return to **Slack** and try queries like:  \n> • “Which customers requested a POC and have transcript_sentiment > 0.5?”  \n> • “List risk_scoring by region for all Gen AI discovery meetings.”  \n\nNow you’ll seamlessly span **structured** + **unstructured** insights in one natural-language interface!"
  }
 ]
}